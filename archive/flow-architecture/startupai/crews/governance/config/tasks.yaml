quality_review:
  description: >
    Perform quality review of validation outputs at gate checkpoints:

    Phase: {phase}
    State: {state}
    Evidence: {evidence}

    Review for:
    1. Completeness - All required outputs present
    2. Accuracy - Calculations and analysis correct
    3. Consistency - No contradictions between sections
    4. Evidence quality - Sufficient data to support conclusions
    5. Actionability - Recommendations are specific and executable
  expected_output: >
    QA report containing:
    - status: PASSED/FAILED/NEEDS_REVISION
    - completeness_score: float 0-1
    - issues_found: list of {severity, description, location}
    - recommendations: list of improvements
    - gate_decision: proceed/revise/escalate
  agent: qa_auditor

final_audit:
  description: >
    Perform comprehensive final audit of complete validation:

    Full State: {full_state}

    Audit all aspects:
    1. Process compliance - Did we follow the methodology?
    2. Evidence chain - Is every conclusion supported?
    3. Pivot rationale - Were pivots evidence-driven?
    4. Output quality - Are deliverables complete and accurate?
    5. Learning capture - Have insights been documented?
  expected_output: >
    Final audit report containing:
    - status: PASSED/FAILED
    - overall_score: float 0-1
    - process_compliance: assessment
    - evidence_quality: assessment
    - key_findings: list
    - improvement_areas: list for future validations
  agent: qa_auditor

track_progress:
  description: >
    Track validation progress and maintain audit trail:

    Validation ID: {validation_id}
    Current Phase: {current_phase}
    Events: {events}

    Record:
    1. Phase transitions with timestamps
    2. Key decisions and rationale
    3. Evidence collected
    4. Pivots executed
    5. Human approvals obtained
  expected_output: >
    Progress report containing:
    - timeline: list of events with timestamps
    - phase_history: phases visited
    - decisions_log: key decisions made
    - evidence_summary: what was collected
    - audit_trail: complete record for compliance
  agent: accountability_tracker

capture_learnings:
  description: >
    Extract and document learnings for the Flywheel system:

    Validation Results: {results}
    Pivot History: {pivot_history}
    Evidence: {evidence}

    Identify:
    1. What worked well
    2. What didn't work
    3. Patterns to recognize
    4. Process improvements
    5. Knowledge to preserve
  expected_output: >
    Learning capture containing:
    - key_learnings: list of insights
    - pattern_matches: similar past validations
    - process_improvements: suggestions
    - knowledge_artifacts: reusable content
    - flywheel_updates: what to add to knowledge base
  agent: accountability_tracker

review_creatives:
  description: >
    Review creative artifacts (landing pages, ad creatives) before deployment:

    Artifacts: {artifacts}
    Business Context: {business_context}
    Campaign Goals: {campaign_goals}

    Using the guardian_review tool, check each artifact for:
    1. Compliance issues (misleading claims, missing disclaimers)
    2. Accessibility (alt text, contrast, screen reader compatibility)
    3. Conversion best practices (CTA clarity, value prop visibility)
    4. Security concerns (unsafe URLs, suspicious scripts)
    5. Brand consistency (tone, messaging alignment)

    Determine which artifacts can be auto-approved and which need human review.
  expected_output: >
    Creative review report containing:
    - review_results: list of {artifact_id, decision, issues}
    - auto_approved: list of artifact IDs safe to deploy
    - needs_human_review: list requiring human approval with reasons
    - rejected: list that must be fixed before any approval
    - summary: overall assessment and recommendations
  agent: qa_auditor

validate_methodology:
  description: >
    Validate strategic artifacts against Strategyzer methodology:

    Methodology Type: {methodology_type}
    Artifact: {artifact}
    Related Context: {context}

    Using the methodology_check tool, validate:
    1. VPC - Customer Profile completeness (jobs, pains, gains)
    2. VPC - Value Map completeness (products, pain relievers, gain creators)
    3. VPC - Fit between customer profile and value map
    4. BMC - All 9 building blocks present and consistent
    5. Assumptions - Properly mapped and prioritized

    Flag any structural issues that would prevent meaningful validation.
  expected_output: >
    Methodology validation report containing:
    - is_valid: boolean indicating structural soundness
    - completeness_score: 0.0-1.0 score
    - quality_score: 0.0-1.0 score
    - issues: list of {severity, component, message, suggestion}
    - ready_for_next_phase: boolean
    - summary: human-readable assessment
  agent: compliance_monitor

retrieve_similar_validations:
  description: >
    Retrieve insights from similar past validations for context:

    Current Industry: {industry}
    Current Stage: {stage}
    Current Phase: {phase}
    Business Description: {business_description}

    Using the flywheel_insights tool, retrieve:
    1. Industry-specific patterns and benchmarks
    2. Stage-appropriate guidance and success criteria
    3. Common mistakes to avoid at this phase
    4. Typical pivots and their outcomes for similar startups
    5. Recommendations based on accumulated learnings

    This context helps make better predictions and recommendations.
  expected_output: >
    Cross-validation context containing:
    - industry_patterns: typical metrics, pivots, warning signs
    - stage_guidance: focus, key questions, success criteria
    - recommendations: contextual suggestions
    - similar_validations: relevant past cases (anonymized)
    - confidence_factors: what to watch for
  agent: qa_auditor

track_predictions:
  description: >
    Record predictions at decision points for outcome tracking:

    Validation ID: {validation_id}
    Decision Point: {decision_point}
    Current Evidence: {evidence}
    Recommended Action: {recommended_action}

    Using the outcome_tracker tool, record:
    1. The prediction being made (proceed/pivot/kill)
    2. Confidence level based on evidence strength
    3. Reasoning for the prediction
    4. Key assumptions that must hold
    5. Expected outcome if prediction is correct

    This enables the Flywheel to learn from outcomes over time.
  expected_output: >
    Prediction record containing:
    - prediction_id: unique identifier for later outcome recording
    - prediction_type: gate_decision, pivot_success, etc.
    - predicted_outcome: what we expect to happen
    - confidence: 0.0-1.0 based on evidence
    - reasoning: why we made this prediction
    - key_assumptions: what must be true
  agent: accountability_tracker

record_outcomes:
  description: >
    Record actual outcomes for past predictions:

    Prediction ID: {prediction_id}
    Time Since Prediction: {time_elapsed}
    Actual Results: {actual_results}

    Using the outcome_tracker tool, record:
    1. The actual outcome that occurred
    2. Whether it matched the prediction
    3. Variance notes explaining any differences
    4. Learnings from this prediction cycle

    This feedback loop improves future prediction accuracy.
  expected_output: >
    Outcome record containing:
    - prediction_id: which prediction this updates
    - actual_outcome: what really happened
    - was_correct: boolean match assessment
    - variance_notes: explanation of differences
    - learning_captured: insights from this cycle
  agent: accountability_tracker

check_privacy:
  description: >
    Check content for privacy violations before Flywheel storage:

    Content: {content}
    Context: {context}
    Compliance Requirements: {compliance}

    Using the privacy_guard tool, check for:
    1. PII (emails, phones, SSNs, credit cards)
    2. Sensitive credentials (API keys, passwords)
    3. Business-sensitive information (revenue, valuation)
    4. High-risk content that could identify individuals
    5. Compliance violations (GDPR, CCPA, HIPAA)

    Content must pass privacy check before being stored in Flywheel.
  expected_output: >
    Privacy check result containing:
    - is_safe: boolean indicating if content can be stored
    - sensitivity_level: public, internal, confidential, or restricted
    - violations: list of detected privacy issues
    - recommendations: how to remediate violations
    - sanitized_content: cleaned version if requested
  agent: compliance_monitor

validate_cross_validation_sharing:
  description: >
    Validate content is safe for cross-validation sharing:

    Content: {content}
    Source Validation: {source_validation_id}
    Target Validation: {target_validation_id}

    Using the privacy_guard tool, ensure:
    1. No founder-specific PII is present
    2. Business details are abstracted to patterns
    3. Content represents generalizable learnings
    4. Privacy boundaries between validations are maintained

    Cross-validation sharing enables Flywheel learning while protecting privacy.
  expected_output: >
    Cross-validation privacy check containing:
    - is_safe: boolean indicating if sharing is permitted
    - violations: any privacy boundary issues
    - recommendations: how to properly abstract content
    - audit_record: compliance trail for the sharing operation
  agent: compliance_monitor
