# StartupAI Validation Backlog

This is not a feature list. It's a queue of **hypotheses to validate** using lean startup methodology: Build ‚Üí Measure ‚Üí Learn.

## How to Use This Document

1. **Pick a hypothesis** based on what you need to learn next
2. **Build the minimum** required to test it
3. **Measure the outcome** with real users
4. **Learn and decide**: Pivot or persevere
5. **Update this document** with learnings

---

## Priority 0: Tool Integration (CRITICAL)

### Hypothesis: Tools Enable Evidence-Based Validation

> **If** we wire tools to all 45 agents using MCP-first architecture,
> **Then** agents will produce evidence-based outputs instead of hallucinations.

**Status**: READY FOR IMPLEMENTATION

**Problem Statement**:
- 36 of 45 agents (80%) lack tools
- Without tools, agents hallucinate: invented market research, fake competitor data, made-up unit economics
- The SAY vs DO evidence hierarchy requires real data collection

**Evidence of Problem**:
```
Current: D2 hallucinates "HR managers struggle with X" (no evidence)
Target:  D2 searches Reddit for real complaints (47 mentions found)
```

**Build Required** (60 hours):

| Phase | Focus | Effort | Status |
|-------|-------|--------|--------|
| A | Customer Research Tools | 7h | ‚úÖ Complete |
| B | Advanced Analysis Tools | 12h | ‚úÖ Complete |
| C | Analytics & Privacy Tools | 9h | ‚úÖ Complete |
| D | CrewAI Final Integration | 18h | Not started |

**Immediate Actions** (7 hours) - ‚úÖ COMPLETE:
1. ‚úÖ Migrate tools from `intake_crew/tools/` to `shared/tools/`
2. ‚úÖ Add MCP deps to Modal image
3. ‚úÖ Wire TavilySearchTool to research agents
4. ‚úÖ Apply IntakeCrew pattern to all agents

**Measure**:
- [x] Phases A-C complete: 12 tools implemented (75% of Phase D remaining)
- [x] 28/45 agents now have tools wired (62%)
- [ ] Phase 1 VPC Discovery uses real search data
- [ ] Phase 2 Desirability deploys real landing pages
- [ ] Fit scores based on evidence, not LLM synthesis

**Learn**:
- Does real data change validation outcomes?
- What's the quality difference between hallucinated vs evidence-based outputs?

---

## Priority 1: E2E Validation

### Hypothesis: Full Pipeline Produces Actionable Results

> **If** we complete Phase 3-4 live testing and tool integration,
> **Then** users will receive actionable pivot/proceed recommendations backed by evidence.

**Status**: BLOCKED by Priority 0 (tool integration)

**Build Required**:
- [ ] Complete Phase 3 (Feasibility) live testing
- [ ] Complete Phase 4 (Viability) live testing
- [ ] Test pivot loopback (approve_segment_pivot ‚Üí Phase 1)
- [ ] E2E validation with tools producing real evidence

**Measure**:
- [ ] Full Phase 0‚Üí4 cycle completes successfully
- [ ] Pivot/proceed decision based on real evidence
- [ ] User can trace decision back to evidence sources

---

## Priority 2: Product App Integration

### Hypothesis: Users Complete Analysis

> **If** we provide an end-to-end flow from onboarding to analysis results display,
> **Then** users will complete the full analysis and find value in the output.

**Status**: BLOCKED by Priority 1 (E2E validation)

**Current State**:
- ‚úÖ CrewAI Modal deployment live
- ‚úÖ `/kickoff`, `/status`, `/hitl/approve` endpoints working
- ‚úÖ Webhook to persist results to Supabase
- ‚ùå Product app doesn't call Modal endpoints yet
- ‚ùå Product app doesn't display validation results

**Build Required** (Product App repo):
- [ ] Wire onboarding complete ‚Üí Modal `/kickoff`
- [ ] Poll `/status` for progress updates
- [ ] Display results from `validation_runs` table
- [ ] Display HITL approval UI

**Measure**:
- [ ] Completion rate: onboarding ‚Üí validation results
- [ ] Time to first result
- [ ] User satisfaction with output quality

---

## ‚úÖ COMPLETED HYPOTHESES

### Hypothesis: Modal Serverless Enables Scale ‚úÖ VALIDATED

> **If** we deploy to Modal serverless with checkpoint-and-resume HITL pattern,
> **Then** we achieve $0 idle costs, platform independence, and simplified deployment.

**Built** (2026-01-08):
- ‚úÖ Modal infrastructure deployed
- ‚úÖ 14 crews, 45 agents implemented
- ‚úÖ 10 HITL checkpoints with checkpoint-and-resume
- ‚úÖ Supabase state persistence + Realtime
- ‚úÖ 185 unit tests passing

**Measured**:
- ‚úÖ $0 idle costs (vs AMP always-on)
- ‚úÖ Single repository (vs 3-repo AMP workaround)
- ‚úÖ Pay-per-second billing
- ‚úÖ Live testing Phase 0-2 passed

**Learned**:
- Modal is ideal for long-running agentic workflows
- Checkpoint-and-resume pattern works well for HITL
- Three-layer architecture (Netlify + Supabase + Modal) provides clean separation

**Status**: ‚úÖ COMPLETE - Modal is production architecture

---

### Hypothesis: Multi-Crew System Enables Scale ‚úÖ VALIDATED

> **If** we structure validation as 5 Flows with 14 Crews and 45 Agents,
> **Then** we can deliver higher quality analysis with clear accountability.

**Built** (Phase 1-3 Complete):
- ‚úÖ 5 Flows orchestrating 14 Crews
- ‚úÖ 45 agents with specialized roles
- ‚úÖ Non-linear routing with Innovation Physics
- ‚úÖ 10 HITL checkpoints

**Measured**:
- ‚úÖ All phases deployed to Modal
- ‚úÖ REST API working
- ‚úÖ 185+ tests passing

**Learned**:
- Crew specialization works well for distinct phases
- Pattern hierarchy (Phase ‚Üí Flow ‚Üí Crew ‚Üí Agent ‚Üí Task) is clear
- Router-based flow enables true non-linear iteration

**Status**: ‚úÖ COMPLETE - All phases implemented

---

### Hypothesis: Tools Improve Analysis Quality ‚úÖ PARTIALLY VALIDATED

> **If** we add web search, financial analysis, and deployment tools to agents,
> **Then** analysis quality will significantly improve beyond pure LLM reasoning.

**Built** (IntakeCrew only):
- ‚úÖ TavilySearchTool wired to S2
- ‚úÖ CustomerResearchTool wired to S2
- ‚úÖ MethodologyCheckTool wired to G1
- ‚ùå Other 40+ agents still lack tools

**Measured**:
- ‚úÖ IntakeCrew produces evidence-based research
- ‚ùå Modal crews produce hallucinated outputs (no tools)

**Learned**:
- Tools dramatically improve output quality
- IntakeCrew pattern is the model for all agents
- 60-hour roadmap to complete tool integration

**Status**: ‚è≥ 11% COMPLETE - Tool integration is Priority 0

---

### Hypothesis: HITL Approvals Prevent Bad Outcomes ‚úÖ IMPLEMENTED

> **If** we require human approval for high-risk actions,
> **Then** users will trust the system with more autonomous operation.

**Built**:
- ‚úÖ 10 HITL checkpoints across 5 phases
- ‚úÖ Checkpoint-and-resume pattern on Modal
- ‚úÖ `/hitl/approve` endpoint
- ‚úÖ Supabase `hitl_requests` table

**Measured**:
- ‚úÖ HITL pauses work correctly
- ‚úÖ Resume continues from checkpoint
- ‚è≥ User trust - PENDING REAL USERS

**Status**: ‚úÖ BACKEND COMPLETE - Frontend UI in Product App pending

---

## Deprioritized (Revisit Later)

### Real-time Streaming
> Users want to watch analysis happen step-by-step.

**Why deprioritized**: Unclear if wanted. Validate core value first.

### Advanced Market Data APIs
> Bloomberg, Crunchbase for competitive intelligence.

**Why deprioritized**: Start with Tavily, add expensive APIs only if needed.

### Public Activity/Metrics APIs
> Show real-time agent activity on marketing site.

**Why deprioritized**: Need production traffic first.

---

## Learnings Log

| Date | Hypothesis | Outcome | Learning | Decision |
|------|------------|---------|----------|----------|
| 2026-01-10 | Phase A-C tools | ‚úÖ 75% Complete | 12 tools, 28 agents, 632 tests | Phase D next |
| 2026-01-09 | MCP-first tools | Ready | Architecture designed, 60h to implement | Proceed |
| 2026-01-08 | Modal serverless | ‚úÖ Validated | $0 idle, single repo, pay-per-second | Proceed |
| 2026-01-07 | Architecture alignment | ‚úÖ Validated | 5 Flows, 14 Crews, 45 Agents canonical | Proceed |
| 2025-11-27 | Multi-crew system | ‚úÖ Validated | Specialization works | Proceed |
| 2025-11-26 | HITL workflow | ‚úÖ Validated | Checkpoint-resume works | Build frontend |

---

## Decision Framework

When choosing what to validate next, ask:

1. **What's the riskiest assumption?** Start there.
2. **What blocks other work?** Unblock it.
3. **What can we learn fastest?** Minimize build time.
4. **What has the biggest impact if true?** Maximize learning value.

**Current Priority**: **Tool Integration** is the critical blocker
- Riskiest: Will tools change validation quality?
- Blocking: All evidence-based validation
- Fastest: 7h immediate actions unblock everything
- Biggest impact: Enables real validation vs hallucination

---

## Implementation Status Summary

| Component | Status | Completion |
|-----------|--------|------------|
| Modal Infrastructure | ‚úÖ Complete | 100% |
| 14 Crews / 45 Agents | ‚úÖ Complete | 100% |
| 10 HITL Checkpoints | ‚úÖ Complete | 100% |
| State Management | ‚úÖ Complete | 100% |
| 185+ Unit Tests | ‚úÖ Complete | 100% |
| Live Testing Phase 0-2 | ‚úÖ Complete | 100% |
| MCP Architecture Design | ‚úÖ Complete | 100% |
| **Tool Integration** | üü° IN PROGRESS | **75%** (Phases A-C done, D pending) |
| Live Testing Phase 3-4 | ‚è≥ Pending | 0% |
| Product App Integration | ‚è≥ Blocked | 0% |

---

**Last Updated**: 2026-01-10

**Latest Changes**:
- Phases A-C tool integration COMPLETE (2026-01-10)
  - 12 tools implemented, 28/45 agents wired
  - 632 tests passing
- Complete rewrite for Modal serverless architecture
- Added Priority 0: Tool Integration (60h roadmap)
- Updated completed hypotheses with Modal validation
- Removed deprecated AMP references
- Updated implementation status to reflect current state
